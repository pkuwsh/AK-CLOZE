"""bilstm_test

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QXJAfBHo4-TXeugM9FiKaxf9Yy1FkxV9

**构建lstm模型**

输出的是一个1维的向量，即填入选项后的句子的得分（概率）
训练时，分别将四个选项填入句子，将计算得出的概率与label进行CE

正确选项的label为1，错误选项的label为0

这里用了dropout做正则化，以防止过拟合
"""

import torch.nn as nn
import torch

class LSTM(nn.Module):

    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):
        # output_dim为4
        super(LSTM, self).__init__()

        # 词嵌入函数，后续调用glove的接口进行嵌入
        self.embedding = nn.Embedding(input_dim, embedding_dim)

        # 双向lstm
        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, bidirectional=True)

        # 有2层，所以fc层的输入维度为hidden_dim的两倍
        self.fc = nn.Linear(hidden_dim*2 , output_dim)

        # 每个神经元有50%的可能性丢弃;参数可改
        self.dropout = nn.Dropout(0.8)

    # 下面是前向传播部分
    def forward(self, article):
        # 进行词嵌入,后续用glove直接代替

        #article=article[0]
        article,_=article
        #print("art",article)
        article=article.cuda()
        #article=article.cuda()
        article.transpose_(0,1)
        #print(article.shape)
        embedded = self.dropout(self.embedding(article))
        embedded=embedded.cuda()
        #print(embedded.shape)
        # 计算出结果
        output, (hidden, cell) = self.lstm(embedded)
        output=output.cuda()
        # 把正向和反向的hidden层连接起来
        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)
        hidden=hidden.cuda()
        #print(hidden.shape)
        # dropout
        hidden = self.dropout(hidden)
        #print(hidden.shape)
        # 过一个线性层
        fc = self.fc(hidden)
        fc=fc.cuda()
        #fc=torch.sigmoid(fc)
        #fc=(torch.tanh_(fc)+1)/2
        m = nn.Softmax(dim=0)
        out=m(fc)
        out=out.cuda()
        #print(type(out))
        #0为按列计算，1为按行计算
        #print("fc",fc.shape)
        #print("fc",fc)
        #print("out",out)
        #返回softmax过后的张量
        return out
